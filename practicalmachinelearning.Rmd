<html>

<head>
<title>Practical Machine Learning</title>
</head>

<body>

## Introduction

Research on human activity recognition research focuses to identify trend and health information from different activities. People take their daily exercise using different exercise tools in order to improve their health. Devices such as Jawbone Up, Nike FuelBand, and Fitbit can be used for collecting large amount of data from an individual while doing exercise.  Further analysis of this large amount of data can tell about the personal activity of an individual with a small cost. Usually enthusiastic individuals use these devices to follow up their health improvement. Further information for instance behavior analysis of the individual can also be measured from this data set. Previous data shows that people regularly measure how much particular they do. In this project, we have used data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. These participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. Details of this data can be found at the website:

http://groupware.les.inf.puc-rio.br/har.

## Given Data Set

The training data for this project are available in the following links and I downloaded them in my working directory:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

## Goal of this project

Goal of this project is to create a prediction model to understand how the participants did the exercise. In this predictor model, performance of individual's assigned exercises participated by six participants will be predict. This is the classe variable of the training set, which classifies the correct and incorrect outcomes into A, B, C, D, and E categories. Exercises are classified as: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). After this exercise we have to made a report that describes how the model for the project was built, its cross validation, expected out of sample error calculation, and the choices made.

## Github Link

My github link is: 
https://github.com/sarwar-rcc/practicalmachinelearning

## rPub Link

http://rpubs.com/msjmo/158809

<!--begin.rcode-->

## Details Steps of Predictor

```{r}
# Load the file
setwd("D:/PhD/corsera/Machine Learning/Assignments")
trainingData = read.csv("pml-training.csv", na.strings=c("", "NA", "NULL"))
testingData = read.csv("pml-testing.csv", na.strings=c("", "NA", "NULL"))
```

```{r}
# We take a quick look at the data and particularly at classe which is the variable we need to predict:
str(trainingData, list.len=15)
```

```{r}
table(trainingData$classe)
```


```{r}
prop.table(table(trainingData$user_name, trainingData$classe), 1)
```

```{r}
prop.table(table(trainingData$classe))
```

```{r}
dim(trainingData)
```

```{r}
dim(testingData)
```

```{r}
train.dena<-trainingData[,colSums(is.na(trainingData)) == 0]
dim(train.dena)
```

```{r}
 remove = c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'new_window', 'num_window')
```
 
```{r}
train.dare <- train.dena[,-which(names(train.dena) %in% remove)]
```
 
```{r}
 dim(train.dare)
```
 
```{r}
 library(lattice)
library(ggplot2)
library(caret)
zeroVar= nearZeroVar(train.dare[sapply(train.dare, is.numeric)], saveMetrics = TRUE)
train.nonzerovar = train.dare[,zeroVar[, 'nzv']==0]
dim(train.nonzerovar)
```

```{r}
corrMatrix <- cor(na.omit(train.nonzerovar[sapply(train.nonzerovar, is.numeric)]))
dim(corrMatrix)
```

```{r}
corrDF <- expand.grid(row = 1:52, col = 1:52)
corrDF$correlation <- as.vector(corrMatrix)
levelplot(correlation ~ row+ col, corrDF)
removecor = findCorrelation(corrMatrix, cutoff = .90, verbose = TRUE)

```

```{r}
train.decor = train.nonzerovar[,-removecor]
dim(train.decor)
```

```{r}
inTrain <- createDataPartition(y=train.decor$classe, p=0.7, list=FALSE)
train <- train.decor[inTrain,]; testing <- train.decor[-inTrain,]
dim(train);
```

```{r}
dim(testing)
```

```{r}
library(tree)
set.seed(12345)
tree.train=tree(classe~.,data=train)
summary(tree.train)
```

```{r}
plot(tree.train)
text(tree.train,pretty=0, cex =.8)
library(caret)
modFit <- train(classe ~ .,method="rpart",data=train)
print(modFit$finalModel)
```

```{r}
library(rattle)
```

```{r}
fancyRpartPlot(modFit$finalModel)
tree.pred=predict(tree.train,testing,type="class")
predMatrix = with(testing,table(tree.pred,classe))
sum(diag(predMatrix))/sum(as.vector(predMatrix))
```

```{r}
tree.pred=predict(modFit,testing)
predMatrix = with(testing,table(tree.pred,classe))
sum(diag(predMatrix))/sum(as.vector(predMatrix))
```

```{r}
cv.train=cv.tree(tree.train,FUN=prune.misclass)
cv.train
```


<!--end.rcode-->


```{r}
plot(cv.train)
```

```{r}
prune.train=prune.misclass(tree.train,best=18)
tree.pred=predict(prune.train,testing,type="class")
predMatrix = with(testing,table(tree.pred,classe))
sum(diag(predMatrix))/sum(as.vector(predMatrix))
```

## Data Manipulation
53 covariates is a lot of variables… Let’s look at their relative importance using the output of a quick Random Forest algorithm (which we call directly using randomForest() rather than the caret package purely for speed purposes as we cannot specify the number of trees to use in caret), and plotting data importance using varImpPlot(). Confusion matrix is also used for getting the idea about accuracy.

```{r}
require(randomForest)
```

```{r}
set.seed(12345)
rf.train=randomForest(classe~.,data=train,ntree=100, importance=TRUE)
rf.train
```


```{r}
varImpPlot(rf.train,)
tree.pred=predict(rf.train,testing,type="class")
predMatrix = with(testing,table(tree.pred,classe))
sum(diag(predMatrix))/sum(as.vector(predMatrix))
```

##Result of the Project Assignment

Finally prediction result  of the project assignment is derived 
```{r}

predictors <- predict(rf.train, testingData)
predictors 
```

## Conclusion
Since I am new in R and machine learning algorithms, I need more time to create a better predictor model.I took help from different prediction model available in online to have better understanding about the  prediction model. And I hope I will do better in the next courses in this data science seriese. 

</body>
</html>
